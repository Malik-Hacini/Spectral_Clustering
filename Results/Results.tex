%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Préambule %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,12pt]{article}
\usepackage{etex}

%%%%%Langue%%%%%

%%\usepackage[utf8]{inputenc}

\usepackage[T1]{fontenc}
\usepackage{babel} 
%%%%Police%%%%

%% MATHS BELLES : \usepackage{mathpazo}
%% TIMES NEW ROMAN :
\usepackage{stmaryrd}
\usepackage{newtxtext, newtxmath}
%%%%%Packages%%%%%

\usepackage[dvipsnames]{xcolor}

\usepackage{afterpage,amsfonts,amsmath,amssymb,amsthm,array,cancel,caption,comment,diagbox,dsfont,enumitem,fancybox,fancyhdr,float, framed,graphics,graphicx,hhline,import,latexsym,lscape,mathabx,multicol,multirow,pdfpages, setspace,subcaption,systeme,tikz,url,xcolor,mdframed}

\usepackage[left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm]{geometry}
\onehalfspacing
\usepackage[french,boxruled]{algorithm2e}
\usepackage[colorlinks=true]{hyperref} 
\usepackage[all]{xy}
\usepackage{dashundergaps}
\usepackage{pgfplots}
\usepackage{pdfpages}
\pgfplotsset{compat=1.18, width=10cm}
\dashundergapssetup{gap-extend-minimum=20pt,gap-extend-percent=20, gap-numbers=false,gap-format=dot,gap-widen=true,teacher-gap-format=dot}

\setlength{\columnsep}{0cm}

\usetikzlibrary{shapes.misc}
\newcommand{\mytimes}{ \tikz[baseline=-.55ex] \node [inner sep=0pt,cross out,draw,line width=1.5pt,minimum size=0.75ex] (a) {};}

\usepackage{tcolorbox}
\tcbuselibrary{theorems}

%%%%Couleurs%%%%

%Version en ligne

\definecolor{t2_red}{RGB}{210,12,70}
\definecolor{t2_blue}{RGB}{12,128,210}
\definecolor{t2_gold}{RGB}{210,180,12}
\definecolor{wooclap_blue}{RGB}{108,151,243}
\definecolor{t2}{RGB}{144,93,146}

%Version Imprimable

%\definecolor{t2_red}{RGB}{0,0,0}
%\definecolor{t2_blue}{RGB}{0,0,0}
%\definecolor{t2_gold}{RGB}{0,0,0}
%\definecolor{wooclap_blue}{RGB}{0,0,0}
%\definecolor{t2}{RGB}{0,0,0}

\hypersetup{urlcolor=t2_red,linkcolor=t2_red,citecolor=t2_red,colorlinks=true}


%%%%%Environnements%%%%%

\newtcbtheorem[number within=section]{defi}{Définition\,}{colback=t2_red!0,colframe=t2_blue,fonttitle=\bfseries}{df}

\newtcbtheorem[number within=section]{prop}{Proposition\,}{colback=t2_red!0,colframe=t2_red,fonttitle=\bfseries}{prop}

\newtcbtheorem[number within=section]{cor}{Corollaire\,}{colback=t2_red!0,colframe=t2_red,fonttitle=\bfseries}{cr}

\newtcbtheorem[number within=section]{theo}{Théorème\,}{colback=t2_red!0,colframe=t2_gold,fonttitle=\bfseries}{th}

\theoremstyle{definition}
%\newtheorem{conv}{Convention}
\newtheorem*{defin}{Définition}
\newtheorem*{ex}{Exemple}
\newtheorem*{exo}{Exercice}
\newtheorem*{corr}{Correction}
\newtheorem*{rap}{Rappel}
\newtheorem*{rem}{Remark}
\newtheorem*{que}{Question}
\newtheorem*{rems}{Remarques}
\theoremstyle{plain}
\newtheorem*{conj}{Conjecture}
\newtheorem*{demo}{Démonstration}
%\newtheorem{cor}{Corollaire}
%\newtheorem{princ}{Principe}
\newtheorem*{lem}{Lemme}
%\newtheorem{prop}{Proposition}
%\newtheorem{theo}{Théorème}
%\newtheorem*{theo_non_numero}{Théorème}

%\declaretheorem[name=Proposition,sibling=prop]{proprep}

%%%%%Macros%%%%%

%%Ensembles%classiques%%

\def\N{\mathbb{N}}%entiers naturels
\def\Z{\mathbb{Z}}%entiers relatifs
\def\Q{\mathbb{Q}}%rationnels
\def\R{\mathbb{R}}%réels
\def\C{\mathbb{C}}%complexes

%%Algèbre%linéaire%%

\def\codim{\mathrm{codim}}%codimension
\def\ker{\mathrm{ker}\,}%noyau
\def\coker{\mathrm{coker}\,}%conoyau
\def\im{\mathrm{im}\,}%image
\def\id{\mathrm{id}}%application identité
\newcommand\scal[2]{\langle #1,#2\rangle}%produit scalaire

%%Géométrie%différentielle%%

\def\grad{\mathrm{grad}}%gradient
\def\ind{\mathrm{ind}\,}%indice
\def\modu{\mathcal{M}}%espace de modules
\def\moduc{\overline{\mathcal{M}}}%espace de modules compactifié
\def\moducbullet{\overline{\mathcal{M}_\bullet}}%espace de modules avec un point marqué compactifié
\def\ev{\mathrm{ev}}%applications évaluations dans la base
\def\Ev{\mathrm{Ev}}%applications évaluations dans l'espace total
\def\lacet{\ell}%lacet de Morse
\def\lacets{\mathcal{L}}%lacets de Morse
\newcommand\ro{roulement}%roulement-crocodile
\newcommand\Star{{S_\star}}%ensemble du point étoilé et des augmentations
\newcommand\fibrEv{\underset{\Ev}{\boxtimes}}%produit fibré des évaluations dans l'espace total
\newcommand\fibrev{\underset{\mathrm{ev}}{\boxtimes}}%produit fibré des évaluations dans la base
\newcommand\moducmix[1]{\overline{\modu^{#1}}}%espace de modules mixtes
\newcommand\moducmixbul[1]{\overline{\modu^{#1}_\bullet}}%espace de modules à rebonds
\newcommand\moducprime{\overline{\mathcal{M}'}}%espace de modules pour un second jeu de données
\newcommand\rg{\mathfrak{r}}
\newcommand\qg{\mathfrak{q}}
\newcommand\Loop{\mathcal{L}}
\newcommand\push{\Phi}
\newcommand\drum{drumroll}

%%%%%Commandes%%%%%

\def\;{\,\,;\,\,}%point virgule espacé

\def\tq{\,\,\mathrm{t.q.}\,\,}%tel que

\def\iff{\Longleftrightarrow}%équivaut à, la commande initiale ne marche pas

\def\implies{\Longrightarrow}%implique, la commande initiale ne marche pas

\newcommand\fonction[5]{#1:
	\begin{array}{ccc}
		#2&\longrightarrow &#3\\
		#4&\longmapsto &#5
\end{array}}%fonction avec ensembles de départ, d'arrivée et transformation

\newcommand\fonctionbira[5]{#1:
	\begin{array}{ccc}
		#2&\dashrightarrow &#3\\
		#4&\longmapsto &#5
\end{array}}

\newcommand\fonctionnonnommee[4]{\begin{array}{ccc}
		#1 &\longrightarrow& #2\\
		#3 &\longmapsto & #4
\end{array}}%fonction avec ensembles de départ, d'arrivée et transformation, sans nom



%%%%%Bibliographie%%%%%

\bibliographystyle{alpha-en}

%%%%%Style%%%%%

\pagestyle{empty}
\pagestyle{fancy}

\setlength{\headheight}{16pt}
\usepackage{listings}

\definecolor{darkWhite}{rgb}{0.94,0.94,0.94}

\lstset{
	aboveskip=3mm,
	belowskip=-2mm,
	backgroundcolor=\color{darkWhite},
	basicstyle=\footnotesize,
	breakatwhitespace=false,
	breaklines=true,
	captionpos=b,
	commentstyle=\color{red},
	deletekeywords={...},
	escapeinside={\%*}{*)},
	extendedchars=true,
	framexleftmargin=16pt,
	framextopmargin=3pt,
	framexbottommargin=6pt,
	frame=tb,
	keepspaces=true,
	keywordstyle=\color{blue},
	language=C,
	literate=
	{²}{{\textsuperscript{2}}}1
	{⁴}{{\textsuperscript{4}}}1
	{⁶}{{\textsuperscript{6}}}1
	{⁸}{{\textsuperscript{8}}}1
	{€}{{\euro{}}}1
	{é}{{\'e}}1
	{è}{{\`{e}}}1
	{ê}{{\^{e}}}1
	{ë}{{\¨{e}}}1
	{É}{{\'{E}}}1
	{Ê}{{\^{E}}}1
	{û}{{\^{u}}}1
	{ù}{{\`{u}}}1
	{â}{{\^{a}}}1
	{à}{{\`{a}}}1
	{á}{{\'{a}}}1
	{ã}{{\~{a}}}1
	{Á}{{\'{A}}}1
	{Â}{{\^{A}}}1
	{Ã}{{\~{A}}}1
	{ç}{{\c{c}}}1
	{Ç}{{\c{C}}}1
	{õ}{{\~{o}}}1
	{ó}{{\'{o}}}1
	{ô}{{\^{o}}}1
	{Õ}{{\~{O}}}1
	{Ó}{{\'{O}}}1
	{Ô}{{\^{O}}}1
	{î}{{\^{i}}}1
	{Î}{{\^{I}}}1
	{í}{{\'{i}}}1
	{Í}{{\~{Í}}}1,
	morekeywords={*,...},
	numbers=left,
	numbersep=10pt,
	numberstyle=\tiny\color{black},
	rulecolor=\color{black},
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	stepnumber=1,
	stringstyle=\color{gray},
	tabsize=4,
	title=\lstname,
}

\title{Evaluation Of Spectral Clustering methods.}
\author{Malik Hacini}
\begin{document}
	\maketitle
	\tableofcontents
	\section{Introduction}
	This document presents the methods, experiments and results achieved for spectral clustering on synthetic and real world datasets. All of the project was written in Python. You will find the code referenced in this presentation in the "src" folder of the project. For more information on spectral clustering, refer to PH.
	\section{Code architecture}
	The code for this project is divided into 2 main Python files :
	\begin{itemize}
		\item graphs.py
		\item spectralclustering.py
	\end{itemize}
	The goal of the structure is to be versatile: to perform a new experiment, you do not need to modify the code, as the different parameters give a lot of flexibility.
	\subsection{graphs.py}
	This file contains everything needed to the construction of a graph from a dataset. There is a "Graph" Python class meant to create a graph based on a list of data points. 
	With class attributes and methods, you can then access all of the spectral clustering related objects of the graph : adjacency matrix and laplacians (with all kinds implemented).
	
	One key feature of these graphs is the similarity function used. A "Similarity" Python class exists for this purpose. However, in most cases , we use the gaussian kernel similarity function, hence it is the default one in our implementation.
	
	Lastly, there is a symmetrizing method parameter. It is the method used for symmetrizing the adjacency matrix in the case of classical spectral clustering. The choices are "mean, or, and". Please refer to the source code for more details.
	Overall, this file is all you need to construct a graph based on your dataset.
	\begin{ex}
		Consider a dataset "data" where we want to build a 10-nn graph G using the gaussian kernel of standard deviation $\frac{1}{2}$ and symmetrizing the adjacency matrix by the 'mean' method, the call would be :
		\begin{lstlisting}
			G=Graph(data,10,'knn','mean',1/2)
		\end{lstlisting}
		We can then easily access the graph adjacency matrix and laplacians using the Graph class attributes and methods.
	\end{ex}
	\subsection{spectralclustering.py}
	This file contains everything needed to perform spectral clustering on a dataset, it's main function being simply called "spectral clustering".
	Understanding this functions's parameters is the only real thing you need to conduct experiments using spectral clustering. Here is the docstring associated with it :
	\begin{lstlisting}
		def spectral_clustering(data,k_neighbors,n_eig,laplacian,g_method='g_knn',sym_method='mean',sigma=None,use_minibatch=False,eigen_only=False,clusters_fixed=False,return_matrix=False,labels_given=np.array([None])):
		"""Performs spectral clustering on a dataset of n-dimensional points.
		Inputs :
		data (ndarray): The dataset, a 
		k_neighbors (int): Number of neighbors you want to connect in the case of a k-nn graph.
		n_eig (int): Number of eigenvectors to calculate. Used to compute the number of clusters
		laplacian (string) : The laplacian to use between [un_norm , sym , rw] 
		sym_method (string): The method used to symmetrize the graph matrix in the case of an asymmetric adjacency matrix.
		sigma (float): Standard deviation for the gaussian kernel.
		use_minibatch (bool) : Choice of the k-means algorithm. True might lead to better performance on large datasets. Default = False.
		eigen_only (bool) : If True, the function will only returns the eigenvalues and eigenvectors and not compute the full clustering. Default = False.
		clusters_fixed (int) : The number of clusters in your data. If unknown, leave by default and the eigengap heuristic will be used. Default = False.
		return_matrix (bool) : True <=> returns the adjacency matrix alongside the clustering results. Use if you want to visualize the graph. Default = False.
		labels_given (ndarray) : The correct labels of your data. If given, used to reorder the labels obtained by clustering. Leave empty if unknown. Default = False 

		Returns :
		vals (ndarray): the computed eigenvalues pf the graph laplacian.
		labels (ndarray) : labels of the points after spectral clustering, ordered in the same way as the dataset.
		matrix (ndarray) : the adjacency matrix of the graph
		"""
	\end{lstlisting}

	
\section{Experiments}
This section summarizes the experiments done.
\subsection{Basic Experiments}
To assert the good behavior of our algorithms, the first step was to conduct very basic experiments. We are going to use very simple toy datasets and focus on 2 important results : Clustering performance and Eigenvalues of laplacians.
\subsubsection{Circles and Moons}
The most basic spectral clustering example are the circles and moons dataset. These datasets fail to be clustered correctly by $k$-means, as the clusters do not lie in disjoint convex sets. However, spectral clustering is powerful enough to correctly cluster these datasets.
Due to the simplicity of the datasets, practically any choice of settings gives perfect performance.
\begin{figure}[H]
	\begin{subfigure}{.6\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/Fig1}
		\caption{Circles}
	\end{subfigure}
	\begin{subfigure}{.6\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/Fig2}
		\caption{Moons}
	\end{subfigure}
	\caption{Comparison between $k$-means and spectral clustering on toy datasets}
\end{figure}

While the settings do not affect performance, the spectra can look significantly different depending on the laplacian we use. Here is a comparison on the circles dataset :

TODO  : ALL LAPLACIANS (INCLUDING GSC) EIGENVALUES 0-4

analysis TODO

\subsubsection{Gaussian Mixture Models (GMMs)}
In the goal of doing qualitative comparison between different methods, using $2$-dimensional gauussian mixture datasets is a good starting point. 
\paragraph{Multivariate Normal Distribution} The multivariate normal distribution, also called "multivariate gaussian" is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions. For any dimension $d$, it can be defined by it's PDF :
$$\fonctionnonnommee{\R^{d}}{\R_{>0}}{x}{\frac{1}{\sqrt{(2\pi)^2 \lvert \Sigma \rvert}} e^{-\frac{1}{2}(x-\mu) \Sigma^{-1} (x-\mu)^{t}}}$$
Where:
\begin{itemize}
	\item $\mu \in{R^d}$ is the mean of the distribution.
	\item $\Sigma$ is a $d$ by $d$ square symmetric positive semi-definite matrix called the covariance matrix, because when $x$ is treated as a random vector with each of it's coordinates $x_i$ being a random variable, $\sigma_{i,j}= Cov(x_{i},x_{j})$.
\end{itemize}
The multivariate gaussian distribution is centered at $\mu$ and has an ellipsoidal shape defined by $\Sigma$.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{figures/Binorm}
	\caption{Level sets of a bivariate normal distribution centered at the origin. The probability of a sample landing in the blue ellipse is $0.66$.}
	\label{fig:binorm}
\end{figure}

A linear combination of $d$-dimensional gaussians is called a gaussian mixture.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{figures/2gm}
	\caption{$200$ samples from a mixture of two 2-dimensional gaussians.}
	\label{fig:2gm}
\end{figure}
\begin{rem} 
	In practice, since gaussians PDF decay quickly when driving away from $\mu$, when the means are fairly spaced, the different gaussians do not really interact to form a diffrent PDF and the resulting mixture just looks like a superpostion of the respective gaussians. Thus, we do not perform the sampling with the actual PDF of the mixture, but uniformly choose between the different gaussians for each point.
\end{rem}
Gaussian mixtures form great toy datasets on which we can test and visualize our clustering, where a goal cluster is all of the points sampled from a particular gaussian.

\begin{figure}[H]
	\captionsetup{justification=centering}
	\begin{subfigure}{.6\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/fig5_g1}
		\caption{Successful clustering}
	\end{subfigure}
	\begin{subfigure}{.6\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/fig6_g2}
		\caption{Unsuccessful clustering}
	\end{subfigure}
	\caption{Visualization of spectral clustering and it's $k$-nn graph on a gaussian mixture. \\ Settings : $L_{rw}$, $k=6$, $\sigma=\frac{1}{3}$ (for gaussian kernel).}
\end{figure}
On figure (a) we see that the $k$-nn graph well encodes the cluster structure of the data. It is fully connected but there is only one link between clusters, which makes for great performance of SC using $L_{rw}$. \\ \\
On figure (b), the means of the two gaussians are closer apart, thus the clustering fails to partition the graph correctly due to the multiple links between the real clusters.
However, increasing $k$ creates stronger links inside of the clusters, leading to better performance : 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{figures/fig7_g3}
	\caption{The performance of the clustering is increased with $k=8$}
	\label{fig:fig7g3}
\end{figure}

\subsection{UCI datasets}
Now that we know our algorithms follow the basic theoretical behaviors we want, we can benchmark them on more interesting datasets.
We used the same 11 benchmark datasets from the UCI repository \cite{UCI} as \cite[Jonckheere et.al]{GSC}. These datasets are high dimensional and to correctly assert performance of the clustering, we need a pertinent quantitative metric.
\subsubsection{Normalized Mutual Information (NMI)}
The NMI facilitates comparisons between two different ways of partitioning a dataset, yielding a value that ranges from 0 to 1. A higher value indicates a greater degree of similarity between partitions. As an external metric, the NMI necessitates the availability of class labels for computations, implying that the ground truth is required when employing this metric.
The calculation of the NMI between two partitions A and B is executed according to the following equation :
$$NMI\left(A,B\right)=\frac{2*I(A,B)}{[H\left(A\right)+H(B)]}$$
	
	Where $I(A,B)$ is the mutual information and $H$ the entropy. 
	In practice, $A$ is the set of predicted labels and $B$ the ground truth labels of the dataset. \\ NMI can fail to correctly assess the performance of the clustering when the number of clusters predicted and the ground truth number of clusters do not match. However, this won't be the case in our experiments, since we will force the clustering to build the right number of clusters.
	Other evaluation metrics such as the adjusted Rand Index (ARI) exist, but we limited ourselves to NMI for these experiments as it already is very well suited for comparison purposes.

\newpage

\begin{thebibliography}{9}
	
	\bibitem{UCI}
	Dheeru, D. and Karra Taniskidou.UCI repository of machine learning databases \textit{University of California, Irvine, School of
	Information and Computer Sciences}, 2017.
	
	\bibitem{GSC}
	Harry Sevi, Matthieu Jonckheere, and Argyris Kalogeratos.
	Generalized spectral clustering for directed and undirected graphs,
	2022.
	
\end{thebibliography}
\end{document}